{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gensim, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "import os\n",
    "import multiprocessing\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shakespeare.txt from Gutenberg open source http://norvig.com/ngrams/\n",
    "\n",
    "# GenSim Word2Vec expects sentence to be fed sequentially, hence this construct for corpus sentences iterator class\n",
    "class GetSentencesFromDir(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname)):\n",
    "                yield line.split()\n",
    " \n",
    "sentences = GetSentencesFromDir('/shakespeare_dir') # a memory-friendly iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gensim.corpora.wikicorpus import WikiCorpus\n",
    "#wiki = WikiCorpus(\"some_wiki_articles_dump.xml.bz2\", lemmatize=False, dictionary={})\n",
    "#sentences = list(wiki.get_texts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-09 17:29:40,985 : INFO : collecting all words and their counts\n",
      "2018-11-09 17:29:40,987 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-11-09 17:29:41,013 : INFO : PROGRESS: at sentence #10000, processed 82874 words, keeping 8218 word types\n",
      "2018-11-09 17:29:41,040 : INFO : PROGRESS: at sentence #20000, processed 159884 words, keeping 12353 word types\n",
      "2018-11-09 17:29:41,067 : INFO : PROGRESS: at sentence #30000, processed 240235 words, keeping 15238 word types\n",
      "2018-11-09 17:29:41,090 : INFO : PROGRESS: at sentence #40000, processed 319260 words, keeping 18133 word types\n",
      "2018-11-09 17:29:41,116 : INFO : PROGRESS: at sentence #50000, processed 394354 words, keeping 20518 word types\n",
      "2018-11-09 17:29:41,145 : INFO : PROGRESS: at sentence #60000, processed 475506 words, keeping 22757 word types\n",
      "2018-11-09 17:29:41,172 : INFO : PROGRESS: at sentence #70000, processed 553869 words, keeping 25222 word types\n",
      "2018-11-09 17:29:41,196 : INFO : PROGRESS: at sentence #80000, processed 635835 words, keeping 26962 word types\n",
      "2018-11-09 17:29:41,222 : INFO : PROGRESS: at sentence #90000, processed 707321 words, keeping 28254 word types\n",
      "2018-11-09 17:29:41,243 : INFO : PROGRESS: at sentence #100000, processed 772627 words, keeping 29714 word types\n",
      "2018-11-09 17:29:41,267 : INFO : PROGRESS: at sentence #110000, processed 846095 words, keeping 31192 word types\n",
      "2018-11-09 17:29:41,288 : INFO : PROGRESS: at sentence #120000, processed 913895 words, keeping 32477 word types\n",
      "2018-11-09 17:29:41,312 : INFO : collected 33505 word types from a corpus of 980637 raw words and 129107 sentences\n",
      "2018-11-09 17:29:41,313 : INFO : Loading a fresh vocabulary\n",
      "2018-11-09 17:29:41,343 : INFO : effective_min_count=2 retains 17786 unique words (53% of original 33505, drops 15719)\n",
      "2018-11-09 17:29:41,346 : INFO : effective_min_count=2 leaves 964918 word corpus (98% of original 980637, drops 15719)\n",
      "2018-11-09 17:29:41,388 : INFO : deleting the raw counts dictionary of 33505 items\n",
      "2018-11-09 17:29:41,390 : INFO : sample=0.001 downsamples 50 most-common words\n",
      "2018-11-09 17:29:41,391 : INFO : downsampling leaves estimated 681618 word corpus (70.6% of prior 964918)\n",
      "2018-11-09 17:29:41,434 : INFO : estimated required memory for 17786 words and 20 dimensions: 11738760 bytes\n",
      "2018-11-09 17:29:41,435 : INFO : resetting layer weights\n",
      "2018-11-09 17:29:41,621 : INFO : training model with 7 workers on 17786 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-11-09 17:29:42,294 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-11-09 17:29:42,308 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-09 17:29:42,309 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-09 17:29:42,315 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-09 17:29:42,317 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-09 17:29:42,318 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-09 17:29:42,321 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-09 17:29:42,322 : INFO : EPOCH - 1 : training on 980637 raw words (681883 effective words) took 0.7s, 978046 effective words/s\n",
      "2018-11-09 17:29:42,985 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-11-09 17:29:42,996 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-09 17:29:42,997 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-09 17:29:43,005 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-09 17:29:43,006 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-09 17:29:43,007 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-09 17:29:43,009 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-09 17:29:43,010 : INFO : EPOCH - 2 : training on 980637 raw words (681629 effective words) took 0.7s, 1002230 effective words/s\n",
      "2018-11-09 17:29:43,012 : INFO : training on a 1961274 raw words (1363512 effective words) took 1.4s, 981385 effective words/s\n"
     ]
    }
   ],
   "source": [
    "params = {'size': 20, 'window': 5, 'min_count': 2, 'workers': max(1, multiprocessing.cpu_count() - 1), 'sample': 1E-3, 'iter': 2}\n",
    "\n",
    "# gensim’s word2vec expects a sequence of sentences as its input\n",
    "# gensim’s word2vec first pass collects words and their frequencies to build an internal dictionary tree structure\n",
    "# Then, iter/epoch passes for training neural network model\n",
    "# Trained model memory requirement is unique_tokens*nn_size*float_size*3_matrices\n",
    "model = Word2Vec(sentences, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17786"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-09 17:18:01,996 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('duke', 0.9445843696594238)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-09 17:18:38,808 : WARNING : vectors for words {'lunch', 'cereal'} are not present in the model, ignoring these words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'dinner'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"breakfast cereal dinner lunch\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9364297"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('woman', 'man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('thing', 0.9505487084388733),\n",
       " ('woman', 0.9364296793937683),\n",
       " ('fool', 0.8970590829849243),\n",
       " ('fellow', 0.8291076421737671),\n",
       " ('maid', 0.8163024187088013),\n",
       " ('gentleman', 0.80623459815979),\n",
       " ('word', 0.804107666015625),\n",
       " ('little', 0.7943037748336792),\n",
       " ('knave', 0.7773409485816956),\n",
       " ('bachelor', 0.771576464176178)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mistress', 0.9322229623794556),\n",
       " ('sister', 0.9245880842208862),\n",
       " ('prince', 0.9186954498291016),\n",
       " ('uncle', 0.9049043655395508),\n",
       " ('captain', 0.8951232433319092),\n",
       " ('nurse', 0.8933846354484558),\n",
       " ('daughter', 0.8857458829879761),\n",
       " ('servant', 0.884211003780365),\n",
       " ('errand', 0.881351113319397),\n",
       " ('mother', 0.88055020570755)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"queen\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

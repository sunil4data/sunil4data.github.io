{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB Movie Reviews Sentiment Classification\n",
    "\n",
    "#### Solution workflow: -\n",
    "\n",
    "* Extract train & test sentences containing 'max_features' most frequent tokens from Keras in-built IMDB dataset (tokens are Positional Index into its vocabulary)... all infrequent tokens are represented by some constant.\n",
    "\n",
    "* Trim/pad sentences to 'sent_max_len' words (same as LSTM input's timesteps param and also the LSTM 'units') https://stackoverflow.com/questions/44273249/in-keras-what-exactly-am-i-configuring-when-i-create-a-stateful-lstm-layer-wi\n",
    "\n",
    "* Define Supervised ML network\n",
    "    * Embedding represents each token as 'word_vec_len' x 1 Word Vector... Not sure what pre-trained embedding does Keras+IMDB deriving??\n",
    "    * LSTM many-to-one takes in batch of sentence, i.e., batch_size x sent_max_len x word_vec_len\n",
    "    * Binary Sigmoid layer for binary classification\n",
    "\n",
    "* Fit model with train & validate (same as test) labeled sentences\n",
    "\n",
    "* Evaluate accuracy with test sentences\n",
    "\n",
    "\n",
    "#### Compare accuracy for different run options\n",
    "\n",
    "* LSTM, 10 epochs: Validation accuracy actually fluctuates b/w 0.83 & 0.85\n",
    "* Bi-di LSTM, 10 epochs: Almost same accuracy range as with just LSTM\n",
    "\n",
    "\n",
    "#### Extending this base solution further: - \n",
    "\n",
    "* Split train into train & valid for better generalized model learning\n",
    "\n",
    "* Multi label classification\n",
    "\n",
    "* Use either of the RNN, LSTM or GRU\n",
    "\n",
    "* Use explicit Embeddings matrix - either existing pre-trained embeddings (Word2Vec, GloVe, etc) or train your own Embeddings with given text corpus using Gensim\n",
    "\n",
    "* Use forward directional LSTM or bi-directional LSTM. Bi-directional is better, e.g., meaning of Teddy in “He said, Teddy bears are on sale” -vs- “He said, Teddy Roosevelt was a great President”\n",
    "\n",
    "* Use STATEFUL LSTM for seq-2-seq use cases (speech synthesis, machine translation, Q&A) - http://philipperemy.github.io/keras-stateful-lstm/ \n",
    "\n",
    "\n",
    "#### Reference: -\n",
    "This solution is adapted from reference example https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequent tokens, i.e., vocab size\n",
    "max_features = 5000\n",
    "\n",
    "# Words / tokens count in Sentence which is also the LSTM input's timesteps param\n",
    "sent_max_len = 100\n",
    "\n",
    "# Batches of setences to feed into neural net\n",
    "batch_size = 32\n",
    "\n",
    "# Word vector length, i.e., dimension of the dense embedding\n",
    "word_vec_len = 64\n",
    "\n",
    "# LSTM output size assuming we are using many as output which will be ultimately connected to dense layer to match up the label classes\n",
    "lstm_out_size = 64\n",
    "\n",
    "# NN training epochs\n",
    "n_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 2494\n"
     ]
    }
   ],
   "source": [
    "mylen = np.vectorize(len)\n",
    "print(mylen(x_train).min(), mylen(x_train).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(mylen(x_train), 200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 100)\n",
      "x_test shape: (25000, 100)\n"
     ]
    }
   ],
   "source": [
    "# Trim long sentences or pad short ones\n",
    "# What is the implication of trimming lonnng setences to max_len\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=sent_max_len)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=sent_max_len)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1415,   33,    6,   22,   12,  215,   28,   77,   52,    5,   14,\n",
       "        407,   16,   82,    2,    8,    4,  107,  117,    2,   15,  256,\n",
       "          4,    2,    7, 3766,    5,  723,   36,   71,   43,  530,  476,\n",
       "         26,  400,  317,   46,    7,    4,    2, 1029,   13,  104,   88,\n",
       "          4,  381,   15,  297,   98,   32, 2071,   56,   26,  141,    6,\n",
       "        194,    2,   18,    4,  226,   22,   21,  134,  476,   26,  480,\n",
       "          5,  144,   30,    2,   18,   51,   36,   28,  224,   92,   25,\n",
       "        104,    4,  226,   65,   16,   38, 1334,   88,   12,   16,  283,\n",
       "          5,   16, 4472,  113,  103,   32,   15,   16,    2,   19,  178,\n",
       "         32])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build LSTM based network...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 64)          320000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 353,089\n",
      "Trainable params: 353,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#'''\n",
    "print('Build LSTM based network...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_features, output_dim=word_vec_len))\n",
    "model.add(LSTM(units=lstm_out_size, input_shape=(sent_max_len, word_vec_len), dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nprint('Build bi-directional LSTM based network model...')\\n\\nmodel = Sequential()\\n\\n# input_length=sent_max_len\\nmodel.add(Embedding(input_dim=max_features, output_dim=word_vec_len))\\nmodel.add(Bidirectional(LSTM(units=lstm_out_size)))\\nmodel.add(Dropout(0.5))\\nmodel.add(Dense(1, activation='sigmoid'))\\n\\nprint(model.summary())\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print('Build bi-directional LSTM based network model...')\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# input_length=sent_max_len\n",
    "model.add(Embedding(input_dim=max_features, output_dim=word_vec_len))\n",
    "model.add(Bidirectional(LSTM(units=lstm_out_size)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 106s 4ms/step - loss: 0.4714 - acc: 0.7738 - val_loss: 0.4245 - val_acc: 0.8109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x225b2641390>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=n_epochs,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 13s 511us/step\n",
      "Test score: 0.42448286710739136\n",
      "Test accuracy: 0.81088\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

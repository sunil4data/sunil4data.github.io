{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate, i.e., train own Embeddings Matrix\n",
    "\n",
    "\n",
    "#### Background: -\n",
    "\n",
    "Word Embeddings contains Word Vectors as Distributed Representations (low dimensional dense form – contrast it with high dimensional 1-hot sparse form) of tokens. Word Embeddings Matrix (N most frequent tokens by K embedding space dimension): It is just a lookup table of tokens’ embedding vector for use in NLP machine learning (can be used in both unsupervised & supervised.\n",
    "\n",
    "NOTE that this low dimension is quite opaque or obscure (somewhat orthogonal to Eigen space as in SVD) & this word embeddings provide means for calculating syntactic & semantic meanings.\n",
    "\n",
    "Word Embeddings are generated through Unsupervised Pre-training, e.g., Word2Vec, GloVe, or this custom Embeddings Matrix.\n",
    "\n",
    "Down below, observe few typical use cases of distance based arithmetic between Tokens, i.e., Word Vectors. Word2Vec. Gensim must be mostly using Cosine or Euclidean distance. Below results might seem vague which is because of small corpus and scaled down training.\n",
    "\n",
    "\n",
    "#### Gensim Word2Vec model training\n",
    "\n",
    "Gensim’s word2vec expects a sequence of corpus sentences as its input. Its first pass collects words and their frequencies to build an internal dictionary tree structure. Then, iter/epoch passes for training neural network model.\n",
    "\n",
    "\n",
    "##### Food for thought: \n",
    "\n",
    "Why not just use Positional Index from corpus vocab/dict?\n",
    "Difference b/w using Positional Index -vs- 1-hot?\n",
    "Difference b/w above token-2-num options -vs- Word Embeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gensim, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "import os\n",
    "import multiprocessing\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shakespeare.txt from Gutenberg open source http://norvig.com/ngrams/\n",
    "\n",
    "# GenSim Word2Vec expects sentence to be fed sequentially, hence this construct for corpus sentences iterator class\n",
    "class GetSentencesFromDir(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname)):\n",
    "                yield line.split()\n",
    "\n",
    "# Feed apporpriate path to folder containing text corpus file(s)\n",
    "sentences = GetSentencesFromDir('./data/text_corpus_shakespeare') # a memory-friendly iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-22 10:58:21,060 : INFO : collecting all words and their counts\n",
      "2018-11-22 10:58:21,152 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-11-22 10:58:21,262 : INFO : PROGRESS: at sentence #10000, processed 82874 words, keeping 8218 word types\n",
      "2018-11-22 10:58:21,312 : INFO : PROGRESS: at sentence #20000, processed 159884 words, keeping 12353 word types\n",
      "2018-11-22 10:58:21,375 : INFO : PROGRESS: at sentence #30000, processed 240235 words, keeping 15238 word types\n",
      "2018-11-22 10:58:21,433 : INFO : PROGRESS: at sentence #40000, processed 319260 words, keeping 18133 word types\n",
      "2018-11-22 10:58:21,485 : INFO : PROGRESS: at sentence #50000, processed 394354 words, keeping 20518 word types\n",
      "2018-11-22 10:58:21,553 : INFO : PROGRESS: at sentence #60000, processed 475506 words, keeping 22757 word types\n",
      "2018-11-22 10:58:21,611 : INFO : PROGRESS: at sentence #70000, processed 553869 words, keeping 25222 word types\n",
      "2018-11-22 10:58:21,668 : INFO : PROGRESS: at sentence #80000, processed 635835 words, keeping 26962 word types\n",
      "2018-11-22 10:58:21,715 : INFO : PROGRESS: at sentence #90000, processed 707321 words, keeping 28254 word types\n",
      "2018-11-22 10:58:21,764 : INFO : PROGRESS: at sentence #100000, processed 772627 words, keeping 29714 word types\n",
      "2018-11-22 10:58:21,819 : INFO : PROGRESS: at sentence #110000, processed 846095 words, keeping 31192 word types\n",
      "2018-11-22 10:58:21,862 : INFO : PROGRESS: at sentence #120000, processed 913895 words, keeping 32477 word types\n",
      "2018-11-22 10:58:21,912 : INFO : collected 33505 word types from a corpus of 980637 raw words and 129107 sentences\n",
      "2018-11-22 10:58:21,912 : INFO : Loading a fresh vocabulary\n",
      "2018-11-22 10:58:21,977 : INFO : effective_min_count=2 retains 17786 unique words (53% of original 33505, drops 15719)\n",
      "2018-11-22 10:58:21,977 : INFO : effective_min_count=2 leaves 964918 word corpus (98% of original 980637, drops 15719)\n",
      "2018-11-22 10:58:22,079 : INFO : deleting the raw counts dictionary of 33505 items\n",
      "2018-11-22 10:58:22,083 : INFO : sample=0.001 downsamples 50 most-common words\n",
      "2018-11-22 10:58:22,088 : INFO : downsampling leaves estimated 681618 word corpus (70.6% of prior 964918)\n",
      "2018-11-22 10:58:22,180 : INFO : estimated required memory for 17786 words and 20 dimensions: 11738760 bytes\n",
      "2018-11-22 10:58:22,184 : INFO : resetting layer weights\n",
      "2018-11-22 10:58:22,612 : INFO : training model with 7 workers on 17786 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-11-22 10:58:23,623 : INFO : EPOCH 1 - PROGRESS: at 60.19% examples, 427597 words/s, in_qsize 13, out_qsize 0\n",
      "2018-11-22 10:58:23,960 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-11-22 10:58:23,966 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-22 10:58:23,967 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-22 10:58:23,971 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-22 10:58:23,973 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-22 10:58:23,983 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-22 10:58:23,988 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-22 10:58:23,989 : INFO : EPOCH - 1 : training on 980637 raw words (681079 effective words) took 1.4s, 498122 effective words/s\n",
      "2018-11-22 10:58:25,017 : INFO : EPOCH 2 - PROGRESS: at 57.15% examples, 398663 words/s, in_qsize 13, out_qsize 0\n",
      "2018-11-22 10:58:25,442 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-11-22 10:58:25,467 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-22 10:58:25,473 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-22 10:58:25,478 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-22 10:58:25,483 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-22 10:58:25,484 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-22 10:58:25,490 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-22 10:58:25,490 : INFO : EPOCH - 2 : training on 980637 raw words (681590 effective words) took 1.5s, 455630 effective words/s\n",
      "2018-11-22 10:58:25,492 : INFO : training on a 1961274 raw words (1362669 effective words) took 2.9s, 473793 effective words/s\n"
     ]
    }
   ],
   "source": [
    "params = {'size': 20, 'window': 5, 'min_count': 2, 'workers': max(1, multiprocessing.cpu_count() - 1), 'sample': 1E-3, 'iter': 2}\n",
    "\n",
    "model = Word2Vec(sentences, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17786"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-22 10:58:25,724 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('majesty', 0.9783057570457458)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-22 10:58:25,952 : WARNING : vectors for words {'lunch', 'cereal'} are not present in the model, ignoring these words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'breakfast'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"breakfast cereal dinner lunch\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9032757"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('woman', 'man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('thing', 0.9198711514472961),\n",
       " ('urine', 0.9072387218475342),\n",
       " ('woman', 0.9032757878303528),\n",
       " ('matter', 0.8991479873657227),\n",
       " ('ass', 0.8979557156562805),\n",
       " ('gentleman', 0.8971355557441711),\n",
       " ('there', 0.8926539421081543),\n",
       " ('fool', 0.8861606121063232),\n",
       " ('indeed', 0.8844612240791321),\n",
       " ('better', 0.8821287751197815)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spirit', 0.9883180260658264),\n",
       " ('dog', 0.982003927230835),\n",
       " ('conscience', 0.9820006489753723),\n",
       " ('servant', 0.9816766381263733),\n",
       " ('soul', 0.9808666706085205),\n",
       " (\"here's\", 0.980603814125061),\n",
       " ('kinsman', 0.9803838729858398),\n",
       " ('hat', 0.9798363447189331),\n",
       " ('name', 0.9795224070549011),\n",
       " ('mistress', 0.9792896509170532)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"queen\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

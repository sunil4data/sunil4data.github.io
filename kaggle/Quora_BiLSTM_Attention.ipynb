{"cells":[{"metadata":{"_uuid":"69a1a1f521d1547c034eabd98204e6b4c2bc9d68"},"cell_type":"markdown","source":"## Quora Insincere Questions Classification - BiLSTM/GRU with Attention -or- CNN with 2D MaxPooling & using All Embeddings\n\n#### Sunil Kumar\n        \n##### Solution workflows: - \n\n* Prepare voabulary & check against pre-training word embeddings vocab for coverage\n* Observe the test dataset questions length histogram for identifying appropriate length (use similar count for LSTM units)\n* Fix max_features as per cleaned corpus vocab\n* Prepare embedding matrix for our vocab (using 3 of the given pretrained embeddings)\n* Prepare input word vectors\n* Define Bidirectional LSTM/GRU with Attention network (using different path per embedding & then concatenate 3 streams before Dense narrowing down) -or- CNN with 2D Max Pool\n* Train & validate with training partitions through Keras Checkpointing callback which saves the model weights corresponding to the best val_accuracy/f1 => NOTE that this problem has its classes unbalanced and hence F1 Score is the basis of evaluation (it is the Evaluation Rule of this competition) => Keras used to support F1 based loss/accuracy/etc but not any more, hence feeding custom F1-loss/accuracy in the epochs iteration as well as Checkpointing system\n* Re-create same raw model, load the saved best model weights and then Fit the test data to predict label\n* Prepare the submission csv\n\n##### Attention Layer in the NLP Neural Network\n\nThis solution does have seq-2-seq but in the intermediate layer of the network. Ultimately, the network is compressed through Pooling layer, compacting Dense layer, etc for the final goal of binary classification. Note that encoder-decoder pattern is not suited to this problem. This is called Additive Attention - refer to http://ruder.io/deep-learning-nlp-best-practices/.\n\nMost often used in sequence-to-sequence models. Without an attention mechanism, your model has to capture the essence of the entire input sequence in a single hidden state. The attention mechanism is simply giving the network access to its internal memory (in this case, previous layer output). The network retrieves a weighted combination of all memory locations. The network learns these Attention weights too.\n\nDecision analysis: -\n* How to make use of given multiple pre-trained embeddings?\n    * Use them to generate separate predictions from some defined n-net. Ideally, same n-net with these different embedding would fare differently which should help determine weight in averaging separate predictions.\n    * Prepare a part-parallel sub-network to accept multiple inputs (one per embedding) and later merge/concatenate towards binary classifier last layer. NOTE that its feasibility has been checked under memory & runtime limitations.\n    * It appears counter intuitive to average separate word embeddings as no correspondence exists between the dimensions (even when all are of same size 300) of separately trained word embedding sets. But this reference http://aclweb.org/anthology/N18-2031 provides support for averaging!\n* Should prediction be derived through probability thresholding against 0.5 or some different threshold is required?\n    * Not knowing optimal values of other hyperparameters (max_features, max_seq_len, lstm_units, batch_size, learning_rate), this threshold appears to be yet another hyperparameter. If others are roughly known, then this should be searched alone.\n    * F1 Score does dependent on conditional probability https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4442797/.\n* Why & how to normalize Embedding matrix? Should the Embedding Matrix be normalized for seq-2-seq attention leanring based sentiment analysis? https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/72893\n    * https://arxiv.org/pdf/1808.06305.pdf In the word embedding field, it is observed that learned word vectors usually share a large mean and several dominant principal components, which prevents word embedding from being isotropic. Word vectors that are isotropically distributed (or uniformly distributed in spatial angles) can be differentiated from each other more easily.\n\n##### Ideas to try: -\n* Address classes imbalance\n    * Over-sampling of minority class using SMOTE (tried but it gave discouraging learning & prediction results)\n    * Specifiy class weights input to Keras model learning\n* Topic modeling based insincerity classification, i.e., sentiment analysis\n* Windowed or localized Attention\n    * Technically, Attention model should have some concept of 'window'! Yes, Stanford NLP confirms my gut feeling and in fact it helps in achieving better BLEU Score for seq-2-seq NMT! Refer to https://nlp.stanford.edu/pubs/emnlp15_attn.pdf .\n* Ensemble Learning: Learn models separately with each of the pre-trained Embeddings and then take weighted average of the those predictions for final prediction estimation"},{"metadata":{"trusted":true,"_uuid":"af5ae5ad4d40530977a9596d4c2cb70e4a9de4d3"},"cell_type":"code","source":"MAX_FEATURES = 50000\n\n# All given pre-trained embeddings have word vec size 300\nEMBED_DIM = 300\n\n# Just ~0.8% of questions are lengthier than 30+ words\nMAX_SEQ_LEN = 30\nLSTM_UNITS = 32\n\nVALID_TRAIN_RATIO = 0.2\n\nBATCH_SIZE = 512\nN_EPOCHS = 5\nLEARNING_RATE = 0.001\n\nF1_threshold = 0.36\n\nCNN_FILTER_SIZES = [3,1,3]\nN_CNN_FILTERS = 32","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40334c6e438e0d4e26d614da858192a4ad54e0ce"},"cell_type":"code","source":"# Ensuring reproducible randomness is of great importance for optimizing/tuning loss/accuracy goals across experiements/runs\n\nfrom numpy.random import seed\nseed(123)\n\nfrom tensorflow import set_random_seed\nset_random_seed(456)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82e6facdf73eb08b60865c6a1d535b0493a5f29f","scrolled":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport operator \nimport re\n\nfrom nltk.corpus import stopwords\nfrom gensim.models import KeyedVectors\nfrom wordcloud import WordCloud\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nimport tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras import optimizers\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, Dropout, LSTM, CuDNNGRU, Bidirectional, GlobalMaxPool1D, MaxPool2D, concatenate\nfrom keras.layers import Reshape, Flatten, Conv2D, MaxPool2D\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras import backend as K\nfrom keras.callbacks import ModelCheckpoint\n\n\nfrom gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, remove_stopwords, strip_short\nfrom tqdm import tqdm\ntqdm.pandas()\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"433e03405edd5753386a9fdd0c0a7e2aa0d4e7e7"},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/train.csv\")\ntest_df =  pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5518a099f503666d8d7455af16ea8220c2f6625f"},"cell_type":"code","source":"(train_df.shape, test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f3bd6ddc8948c758535802ce6bdcc67f7ddad8b"},"cell_type":"code","source":"def build_vocab(questions, verbose=True):\n    vocab={}\n    \n    for question in tqdm(questions):\n        for word in question:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7478e6eb1132501c325cb061351aeff3b3660ad1"},"cell_type":"code","source":"# Raw vocab\n\nqs_train = train_df[\"question_text\"].apply(lambda x: x.split()).values\nqs_test = test_df[\"question_text\"].apply(lambda x: x.split()).values\nvocab_raw = build_vocab(list(qs_train) + list(qs_test))\n\nvocab_raw_size = len(vocab_raw) + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e1e4ef4bbe1c6d4fa55fc7c6bad9587e3def3aa"},"cell_type":"code","source":"# https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing/notebook\ncontraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\nmispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\npunct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }    \n\ndef clean_text(x):\n    for dic in [contraction_mapping, mispell_dict, punct_mapping]:\n        for word in dic.keys():\n            x = x.replace(word, dic[word])\n    return x\n\ntrain_df['question_text'] = train_df['question_text'].progress_apply(lambda x: clean_text(x))\ntest_df['question_text'] = test_df['question_text'].progress_apply(lambda x: clean_text(x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfe36959752752522191ada0b6f07dfe1c1331d2"},"cell_type":"code","source":"# Keeping numbers and replacing them by ### because it is present in pretrained embedding may not be useful for insincerety differentiation\n# Gensim preprocess_string\n\ntxt_filters = [lambda x: x.lower(), strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, remove_stopwords, strip_short]\nqs_train = train_df[\"question_text\"].apply(lambda x: preprocess_string(x, txt_filters))\nqs_test = test_df[\"question_text\"].apply(lambda x: preprocess_string(x, txt_filters))\n\ntrain_df[\"question_text\"] = qs_train.apply(lambda x: \" \".join(x))\ntest_df[\"question_text\"] = qs_test.apply(lambda x: \" \".join(x))\n\n# For prediction probability threshold search\n_, val_df = train_test_split(train_df, test_size=VALID_TRAIN_RATIO, random_state=12345)\n\n# +1 for missing token, just in case this remains smaller than MAX_FEATURES\nvocab = build_vocab(list(qs_train) + list(qs_test))\nvocab_size = len(vocab) + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fea5ffd48a7bc4b91a545ab4ce27a631d609c093"},"cell_type":"code","source":"train_insincere_qs = train_df[train_df.target == 1].question_text\ntrain_sincere_qs = train_df[train_df.target == 0].question_text\ntest_qs = test_df.question_text\n\nqs_train_insincere = train_insincere_qs.apply(lambda x: x.split()).values\nqs_train_sincere = train_sincere_qs.apply(lambda x: x.split()).values\nqs_test = test_qs.apply(lambda x: x.split()).values\n\nvocab_train_insincere = build_vocab(list(qs_train_insincere))\nvocab_train_sincere = build_vocab(list(qs_train_sincere))\nvocab_test = build_vocab(list(qs_test))\n\n(len(vocab_train_sincere), len(vocab_train_insincere), len(vocab_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d84dcc34a203e91a5dffd11fde7bf7daff952072"},"cell_type":"code","source":"# It seems that the huge amount of remaining keys in Sincere vocab are all mis-spelt & non-english words :)\n#diff = set(vocab_train_sincere.keys()) - set(vocab_test.keys())\n\n# Insincere train & test questions should be just enough for use as Vocab\ngood_vocab_keys = set(vocab_train_insincere.keys()).union(set(vocab_test.keys()))\ngood_vocab_size = len(good_vocab_keys)\n\nmax_tokens = MAX_FEATURES if vocab_size >= MAX_FEATURES else vocab_size\n(MAX_FEATURES, max_tokens, vocab_size, vocab_raw_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4bd55473a7dcf4f27eb5d499272ad41c11ec20c0"},"cell_type":"code","source":"# Insincere questions are 93.8%\n\nsns.countplot(x='target', data=train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a90a6901ee03fccda9faa6005c9636103800cc7d"},"cell_type":"code","source":"def check_coverage(vocab, embeddings_ref):\n    a = {}\n    oov = {}\n    k = 0\n    i = 0\n    for word in tqdm(vocab):\n        try:\n            #a[word] = word2vecDict[word]\n            a[word] = embeddings_ref[word]\n            k += vocab[word]\n        except:\n\n            oov[word] = vocab[word]\n            i += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n\n    return sorted_x\n\n# word_index = tokenizer.word_index\n# NOTE that due to known issue in Keras Tokenizer, num_words & len(word_index) are not same\ndef load_embeddings(word_index, embed_file, max_features):\n\n    embeddings_ref = {}\n    \n    _, file_extension = os.path.splitext(embed_file)\n    if file_extension == '.bin':\n        embeddings_ref = KeyedVectors.load_word2vec_format(embed_file, binary=True)\n    else:\n        def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n        embeddings_ref = dict(get_coefs(*o.split(\" \")) for o in open(embed_file, encoding=\"utf8\", errors='ignore'))\n        \n    embedding_matrix = np.zeros((max_features, EMBED_DIM))\n    \n    for word, i in word_index.items():\n        if file_extension == '.bin':\n            embedding_vector = embeddings_ref[word]\n        else:\n            embedding_vector = embeddings_ref.get(word)\n        \n        if i >= max_features: continue\n        \n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            \n    oov = check_coverage(word_index, embeddings_ref)\n            \n    return embedding_matrix\n\nclass Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06a2af770ca426243c0989935fddbceae8696bac"},"cell_type":"code","source":"train_X = train_df[\"question_text\"].values\nval_X = val_df[\"question_text\"].values\ntest_X = test_df[\"question_text\"].values\n\nval_y = val_df[\"target\"].values\ntrain_y = train_df[\"target\"].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e20071144800c62e90542f449d84081897c123a"},"cell_type":"code","source":"# Feed all questions from train, val & test\n\ntokenizer = Tokenizer(num_words=max_tokens)\ntokenizer.fit_on_texts(list(train_X) + list(test_X))\n#tokenizer.fit_on_texts(vocab_train_insincere.keys(), vocab_test.keys())\n#tokenizer.fit_on_texts(list(train_X) + list(val_X) + list(test_X))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2fb285b059df6f19710aa05611b3c41fa3db7929"},"cell_type":"code","source":"#tokenizer.fit_on_texts(list(vocab_train_insincere.keys()) + list(vocab_test.keys()))\nword_index = tokenizer.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5de9059106409b17a610d7a62ce19b9f3c5cc654"},"cell_type":"code","source":"# All 4 embeddings have word vec size as 300\n\n# TODO: Fix its parsing\n#embeddings_path = \"../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin\"\n#embeddings_gnews = load_embeddings(word_index, embeddings_path, max_tokens)\n\nembeddings_path = \"../input/embeddings/glove.840B.300d/glove.840B.300d.txt\"\nembeddings_glove = load_embeddings(word_index, embeddings_path, max_tokens)\nemb_mean = np.mean(embeddings_glove,axis = 0)\nemb_std = np.std(embeddings_glove, axis = 0)\nembeddings_glove = (embeddings_glove - emb_mean) / emb_std\nprint(\"Done preparing embeddings matrix from pre-trained GloVe!\")\n\nembeddings_path = \"../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt\"\nembeddings_para = load_embeddings(word_index, embeddings_path, max_tokens)\nemb_mean = np.mean(embeddings_para,axis = 0)\nemb_std = np.std(embeddings_para, axis = 0)\nembeddings_para = (embeddings_para - emb_mean) / emb_std\nprint(\"Done preparing embeddings matrix from pre-trained Paragram!\")\n\nembeddings_path = \"../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec\"\nembeddings_wiki = load_embeddings(word_index, embeddings_path, max_tokens)\nemb_mean = np.mean(embeddings_wiki,axis = 0)\nemb_std = np.std(embeddings_wiki, axis = 0)\nembeddings_wiki = (embeddings_wiki - emb_mean) / emb_std\nprint(\"Done preparing embeddings matrix from pre-trained WikiNews!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5263bce226c35eaaaa401a184b68b489d33cb32e"},"cell_type":"code","source":"train_X = tokenizer.texts_to_sequences(train_X)\nval_X = tokenizer.texts_to_sequences(val_X)\ntest_X = tokenizer.texts_to_sequences(test_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee03604b34e60d33e574b8dfd92f8f989ad28d1b"},"cell_type":"code","source":"mylen = np.vectorize(len)\n\nlen_train = mylen(train_X)\nlen_test = mylen(test_X)\n\n# Keep MAX_SEQ_LEN at 30 as 30+ test questions are just 0.03%\n(sum(len_train > 30)*100/train_df.shape[0], sum(len_test > 30)*100/test_df.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cbff0fad234226123eb323e8d07ba0d03e6a6994"},"cell_type":"code","source":"# NOTE that Seaborn distplot does not support log scale :(\n\nsns.distplot( mylen(train_X) , kde=False, color=\"skyblue\", label=\"train_X\")\nsns.distplot( mylen(test_X) , kde=False, color=\"green\", label=\"test_X\")\n\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2a90313864484972f011e5e0619c5753e4923e6"},"cell_type":"code","source":"train_X = pad_sequences(train_X, maxlen=MAX_SEQ_LEN)\nval_X = pad_sequences(val_X, maxlen=MAX_SEQ_LEN)\ntest_X = pad_sequences(test_X, maxlen=MAX_SEQ_LEN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"521adf23c8aa603f164e05ef27d988a92fedfe9f"},"cell_type":"code","source":"# Result was highly disocouraging\n'''\nfrom imblearn.over_sampling import SMOTE\n \nsmote = SMOTE(kind = \"regular\")\ntrain_X, train_y = smote.fit_sample(train_X, train_y)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"663bf8bfbb989d89ac6f5a3b4204ff388822caa7"},"cell_type":"code","source":"#def model_CNN_MaxPool2D(embed_wiki):    \ndef model_CNN_MaxPool2D(embed_glove, embed_para, embed_wiki):  \n    inp = Input(shape=(MAX_SEQ_LEN, ))\n    \n    x_glove = Embedding(max_tokens, EMBED_DIM, weights=[embed_glove])(inp)\n    x_para = Embedding(max_tokens, EMBED_DIM, weights=[embed_para])(inp)\n    x_wiki = Embedding(max_tokens, EMBED_DIM, weights=[embed_wiki])(inp)\n    \n    # Seq of word vec presented as 1-channel tensor volume (like gray in image CNN) MAX_SEQ_LEN rows x EMBED_DIM columns (each row here represent a token of input Seq)\n    x_glove = Reshape((MAX_SEQ_LEN, EMBED_DIM, 1))(x_glove)\n    x_para = Reshape((MAX_SEQ_LEN, EMBED_DIM, 1))(x_para)\n    x_wiki = Reshape((MAX_SEQ_LEN, EMBED_DIM, 1))(x_wiki)\n        \n    conv_glove = Conv2D(N_CNN_FILTERS, kernel_size=(CNN_FILTER_SIZES[0], EMBED_DIM), kernel_initializer='he_normal', activation='tanh')(x_glove)\n    conv_para = Conv2D(N_CNN_FILTERS, kernel_size=(CNN_FILTER_SIZES[1], EMBED_DIM), kernel_initializer='he_normal', activation='tanh')(x_para)\n    conv_wiki = Conv2D(N_CNN_FILTERS, kernel_size=(CNN_FILTER_SIZES[2], EMBED_DIM), kernel_initializer='he_normal', activation='tanh')(x_wiki)\n    \n    maxpool_glove = MaxPool2D(pool_size=(MAX_SEQ_LEN - CNN_FILTER_SIZES[0] + 1, 1))(conv_glove)\n    maxpool_para = MaxPool2D(pool_size=(MAX_SEQ_LEN - CNN_FILTER_SIZES[1] + 1, 1))(conv_para)\n    maxpool_wiki = MaxPool2D(pool_size=(MAX_SEQ_LEN - CNN_FILTER_SIZES[2] + 1, 1))(conv_wiki)\n        \n    z = concatenate([maxpool_glove, maxpool_para, maxpool_wiki], axis=1)\n    z = Flatten()(z)\n    z = Dropout(0.1)(z)\n        \n    outp = Dense(1, activation=\"sigmoid\")(z)\n    \n    model = Model(inputs=inp, outputs=outp)\n\n    # Decay rate momentum rates (beta_1=0.9, beta_2=0.999), epsilon=None, decay=0.0, amsgrad=False\n    adam = optimizers.Adam(lr=LEARNING_RATE)\n    \n    model.compile(optimizer=adam, loss=f1_loss, metrics=['accuracy', f1])\n    #model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model\n\n# CuDNNGRU is ~6-7 times faster than GRU/LSTM on GPU\n#def model_BidiGruLstm_Attention(embed_glove, embed_para, embed_wiki):\ndef model_BidiGruLstm_Attention(embed_wiki):\n    inp = Input(shape=(MAX_SEQ_LEN,))\n    \n    #x_gnews = Embedding(max_tokens, EMBED_DIM, weights=[embed_gnews])(inp)\n    x_glove = Embedding(max_tokens, EMBED_DIM, weights=[embed_glove])(inp)\n    x_para = Embedding(max_tokens, EMBED_DIM, weights=[embed_para])(inp)\n    x_wiki = Embedding(max_tokens, EMBED_DIM, weights=[embed_wiki])(inp)\n    \n    #x_gnews = Bidirectional(CuDNNGRU(LSTM_UNITS, return_sequences=True))(x_gnews)\n    x_glove = Bidirectional(CuDNNGRU(LSTM_UNITS, return_sequences=True))(x_glove)\n    x_para = Bidirectional(CuDNNGRU(LSTM_UNITS, return_sequences=True))(x_para)\n    x_wiki = Bidirectional(CuDNNGRU(LSTM_UNITS, return_sequences=True))(x_wiki)\n    \n    #x_gnews = Attention(MAX_SEQ_LEN)(x_gnews)\n    x_glove = Attention(MAX_SEQ_LEN)(x_glove)\n    x_para = Attention(MAX_SEQ_LEN)(x_para)\n    x_wiki = Attention(MAX_SEQ_LEN)(x_wiki)\n    \n    x = concatenate([x_glove, x_para, x_wiki])\n    \n    x = Dense(16, activation=\"relu\")(x)\n    x = Dropout(0.1)(x)\n    \n    x = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    \n    # Decay rate momentum rates (beta_1=0.9, beta_2=0.999), epsilon=None, decay=0.0, amsgrad=False\n    adam = optimizers.Adam(lr=LEARNING_RATE)\n    \n    model.compile(optimizer=adam, loss=f1_loss, metrics=['accuracy', f1])\n    #model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model  \n\ndef f1(y_true, y_pred):\n    \n    # Adaptation of the \"round()\" used before to get the predictions. Clipping to make sure that the predicted raw values are between 0 and 1.\n    y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), F1_threshold), K.floatx())\n    #y_pred = K.round(y_pred)\n    \n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp / (tp + fp + K.epsilon())\n    r = tp / (tp + fn + K.epsilon())\n\n    f1 = 2*p*r / (p+r+K.epsilon())\n    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n    return K.mean(f1)\n\ndef f1_loss(y_true, y_pred):\n    \n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp / (tp + fp + K.epsilon())\n    r = tp / (tp + fn + K.epsilon())\n\n    f1 = 2*p*r / (p+r+K.epsilon())\n    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n    return 1 - K.mean(f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7bfc58b88c35a4a64bd56fe664c9fbbd6c4dfd66"},"cell_type":"code","source":"from pathlib import Path\n\ntmp_path = Path(\"../tmp\")\nif not tmp_path.is_dir():\n    os.mkdir(tmp_path)\n\nfilepath = \"../tmp/weights_best.hdf5\"\n\n# Check print(model.metrics_names)\ncheckpoint = ModelCheckpoint(filepath, monitor='val_f1', verbose=1, save_best_only=True, mode='max')\n#checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\ncallbacks_list = [checkpoint]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"0b86fffd1d5445a243f8c8f7a0c0177c6ae83d2f"},"cell_type":"code","source":"#model = model_BidiGruLstm_Attention(embeddings_wiki)\nmodel = model_CNN_MaxPool2D(embeddings_glove, embeddings_para, embeddings_wiki)\n\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44cfa0fd113b0f37b5e579661056a6d42039cb22","scrolled":true},"cell_type":"code","source":"#model.fit(train_X, train_y, validation_data=(val_X, val_y), epochs=N_EPOCHS, batch_size=BATCH_SIZE, verbose=1)\n\nmodel_fitting_history = model.fit(train_X, train_y, validation_split=VALID_TRAIN_RATIO, epochs=N_EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks_list, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"745a9be0e564fe17d569f88286dd8014834c46e6"},"cell_type":"code","source":"'''\npred_val_y = model.predict([val_X], batch_size=BATCH_SIZE, verbose=1)\nfor thresh in np.arange(0.1, 0.701, 0.01):\n    thresh = np.round(thresh, 2)\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_val_y>thresh).astype(int))))\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d4143ea1d64758933228d8b4e85370babb7d93c"},"cell_type":"code","source":"plt.figure(1)\nplt.subplot(311)\nplt.plot(np.arange(1, N_EPOCHS+1), model_fitting_history.history['f1'])\nplt.plot(np.arange(1, N_EPOCHS+1), model_fitting_history.history['val_f1'])\nplt.title('model f1 -vs- val_f1')\nplt.ylabel('F1 Score')\nplt.xlabel('epoch')\nplt.legend(['f1', 'val_f1'], loc='upper left')\n\nplt.figure(2)\nplt.subplot(312)\nplt.plot(np.arange(1, N_EPOCHS+1), model_fitting_history.history['loss'])\nplt.plot(np.arange(1, N_EPOCHS+1), model_fitting_history.history['val_loss'])\nplt.title('model loss -vs- val_loss')\nplt.ylabel('Loss')\nplt.xlabel('epoch')\nplt.legend(['loss', 'val_loss'], loc='upper left')\n\nplt.figure(3)\nplt.subplot(313)\nplt.plot(np.arange(1, N_EPOCHS+1), model_fitting_history.history['acc'])\nplt.plot(np.arange(1, N_EPOCHS+1), model_fitting_history.history['val_acc'])\nplt.title('model acc -vs- val_acc')\nplt.ylabel('Accuracy')\nplt.xlabel('epoch')\nplt.legend(['acc', 'val_acc'], loc='upper left')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a48a35a331b66657f776d4114d8a6f32fd3ddcd3"},"cell_type":"code","source":"# Recreate the original network model without above learned weights, so that best learned weights can be loaded into it for Inference, i.e., Prediction\n\n#model_final = model_BidiGruLstm_Attention(embeddings_wiki)\nmodel_final = model_CNN_MaxPool2D(embeddings_glove, embeddings_para, embeddings_wiki)\nmodel_final.load_weights(\"../tmp/weights_best.hdf5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92020ba22d7817ed8f835c1c1d0e1167d861a39d"},"cell_type":"code","source":"pred_test_y = model_final.predict([test_X], batch_size=BATCH_SIZE, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d192dc0fb1ea54a6201123df7050f5fd044aa04"},"cell_type":"code","source":"pred_test_y = (pred_test_y > F1_threshold).astype(int)\n#pred_test_y = np.round(pred_test_y).astype(int)\nout_df = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\nout_df['prediction'] = pred_test_y\nout_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21f3e985a8a36ef86bd3c89626486fb577a7827f"},"cell_type":"markdown","source":"##### Reference\n\n* Some of the pre-processing code has been borrowed from https://www.kaggle.com/kalyankkr/quora-questions-insincere-words "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}
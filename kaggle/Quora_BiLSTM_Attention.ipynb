{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "69a1a1f521d1547c034eabd98204e6b4c2bc9d68"
   },
   "source": [
    "## Quora Insincere Questions Classification - Keras Embedding & BiLSTM\n",
    "\n",
    "#### Sunil Kumar\n",
    "        \n",
    "https://www.kaggle.com/suniliitb96/qiqc-with-bilstm-attention\n",
    "        \n",
    "##### Solution workflows: - \n",
    "\n",
    "* Text cleaning using Gensim to remove tags, punctuation, multiple_whitespaces, numeric, stopwords, short sentences (< 3 words)\n",
    "* Prepare voabulary & check against pre-training word embeddings vocab for coverage\n",
    "* Observe the distribution of test dataset questions length histogram distribution for identifying appropriate length (use similar count for LSTM units)\n",
    "* Fix max_features as per cleaned corpus vocab\n",
    "* Prepare embedding matrix for our vocab\n",
    "* Prepare input word vectors\n",
    "* Define Bidirectional LSTM/GRU with Attention network\n",
    "* Train & validate with training partitions through Keras Checkpointing callback which saves the model weights corresponding to the best val_accuracy\n",
    "* Re-create same raw model, load the saved best model weights and then Fit the test data to predict label\n",
    "* Prepare the submission csv\n",
    "\n",
    "##### Attention Layer in the NLP Neural Network\n",
    "\n",
    "This solution does have seq-2-seq but in the intermediate layer of the network. Ultimately, the network is compressed through Pooling layer, compacting Dense layer, etc for the final goal of binary classification. Note that encoder-decoder pattern is not suited to this problem. This is called Additive Attention - refer to http://ruder.io/deep-learning-nlp-best-practices/.\n",
    "\n",
    "Most often used in sequence-to-sequence models. Without an attention mechanism, your model has to capture the essence of the entire input sequence in a single hidden state. The attention mechanism is simply giving the network access to its internal memory (in this case, previous layer output). The network retrieves a weighted combination of all memory locations. The network learns these Attention weights too.\n",
    "\n",
    "##### Keras Embedding\n",
    "\n",
    "https://stats.stackexchange.com/questions/324992/how-the-embedding-layer-is-trained-in-keras-embedding-layer\n",
    "\n",
    "Keras Embedding layer is just like any other in neural network (if we are not using any external pre-trained embeddings matrix like Word2Vec, GloVe, etc)! It participates with all other layers in the overall neural network for learning to optimize the end goal, i.e., minimize the loss! => I could not locate any official documentation on this \n",
    "\n",
    "It is completely different from Word2Vec or other pre-trained learning network. The Word2Vec refers to a very specific network setup (2 layer shallow along with few other optimizations) which tries to learn an embedding which captures the semantics of words.\n",
    "\n",
    "##### Ideas to try: -\n",
    "* Topic modeling based insincerity classification, i.e., sentiment analysis\n",
    "* Windowed or localized Attention\n",
    "* CNN & 2D Max Pooling\n",
    "* Ensemble Learning: Learn models separately with each of the pre-trained Embeddings and then take weighted average of the those predictions for final prediction estimation\n",
    "\n",
    "##### Things to explore: -\n",
    "* Technically, Attention model should have some concept of 'window'! Yes, Stanford NLP confirms my gut feeling and in fact it helps in achieving better BLEU Score for seq-2-seq NMT! Refer to https://nlp.stanford.edu/pubs/emnlp15_attn.pdf . Need to update Attention to use local window.\n",
    "* Should the Embedding Matrix be normalized for seq-2-seq attention leanring based sentiment analysis? https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/72893 \n",
    "    * https://arxiv.org/pdf/1808.06305.pdf In the word embedding field, it is observed that learned word vectors usually share a large mean and several dominant principal components, which prevents word embedding from being isotropic. Word vectors that are isotropically distributed (or uniformly distributed in spatial angles) can be differentiated from each other more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "af5ae5ad4d40530977a9596d4c2cb70e4a9de4d3"
   },
   "outputs": [],
   "source": [
    "# Input to the Keras Embedding layer for learning on-the-fly word embedding\n",
    "# Unlike pre-trained embedding where embedding dimension (word vector size) is fixed, here user can choose embedding dimention\n",
    "EMBED_DIM = 300\n",
    "\n",
    "MAX_FEATURES = 100000\n",
    "\n",
    "# Just ~0.8% of questions are lengthier than 30+ words\n",
    "MAX_SEQ_LEN = 60\n",
    "LSTM_UNITS = 64\n",
    "\n",
    "VALID_TRAIN_RATIO = 0.1\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "N_EPOCHS = 10\n",
    "LEARNING_RATE = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "82e6facdf73eb08b60865c6a1d535b0493a5f29f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Dropout, LSTM, CuDNNGRU, Bidirectional, GlobalMaxPool1D, MaxPool2D\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, remove_stopwords, strip_short\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "433e03405edd5753386a9fdd0c0a7e2aa0d4e7e7"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/train.csv\")\n",
    "test_df =  pd.read_csv(\"../input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5518a099f503666d8d7455af16ea8220c2f6625f"
   },
   "outputs": [],
   "source": [
    "(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ed4db92322be15a82960cfe82181c2cd0c214d5c"
   },
   "outputs": [],
   "source": [
    "def build_vocab(questions, verbose=True):\n",
    "    vocab={}\n",
    "    \n",
    "    for question in tqdm(questions, disable=(not verbose)):\n",
    "        for word in question:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "48698965f23bfb899a0916393dc9a94fb41a5b7c"
   },
   "outputs": [],
   "source": [
    "questions = train_df[\"question_text\"].progress_apply(lambda x: x.split()).values\n",
    "vocab_raw = build_vocab(questions)\n",
    "\n",
    "vocab_raw_size = len(vocab_raw) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bfe36959752752522191ada0b6f07dfe1c1331d2"
   },
   "outputs": [],
   "source": [
    "# 188878 as vocab_size after cleanup -vs- 508824 original vocab_raw_size\n",
    "\n",
    "txt_filters = [lambda x: x.lower(), strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, remove_stopwords, strip_short]\n",
    "train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: ' '.join(preprocess_string(x, txt_filters)))\n",
    "test_df[\"question_text\"] = test_df[\"question_text\"].progress_apply(lambda x: ' '.join(preprocess_string(x, txt_filters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d84dcc34a203e91a5dffd11fde7bf7daff952072"
   },
   "outputs": [],
   "source": [
    "questions = train_df[\"question_text\"].progress_apply(lambda x: x.split()).values\n",
    "vocab = build_vocab(questions)\n",
    "\n",
    "vocab_size = len(vocab) + 1\n",
    "max_tokens = MAX_FEATURES if vocab_size >= MAX_FEATURES else vocab_size\n",
    "\n",
    "print({k: vocab[k] for k in list(vocab)[:5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aa7d46820011df5bd8362bfe5d4e9fdbd65ff245"
   },
   "outputs": [],
   "source": [
    "vocab_sorted = sorted(vocab.items(), key=lambda kv: kv[1], reverse=True)\n",
    "#vocab_sorted = sorted(vocab.items(), key=lambda kv: kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2765d7da346b06b9dd820eeb25fc53c292513b44"
   },
   "outputs": [],
   "source": [
    "vocab_part = dict((k, v) for k, v in vocab.items() if v <= 100)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c80c00df4d936f54db57e09f0e938809065e23ab"
   },
   "outputs": [],
   "source": [
    "vocab_sorted[19990:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "df947390f111cdd000e349012fbae69867d2e0ce"
   },
   "outputs": [],
   "source": [
    "x = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\n",
    "sorted_by_value = sorted(x.items(), key=lambda kv: kv[1])\n",
    "sorted_by_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d9e3780a94f2df5fec06837f937a1af2fde0e779"
   },
   "outputs": [],
   "source": [
    "iterator = iter(vocab_part.items())\n",
    "for i in range(100):\n",
    "    print(next(iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cb08ed5c0b852cf867c0c52a30b5554d077c3a27"
   },
   "outputs": [],
   "source": [
    "(MAX_FEATURES, max_tokens, vocab_size, vocab_raw_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c2021f5bb1a9050ea6d4d3a957a1fa3ab3f9930d"
   },
   "outputs": [],
   "source": [
    "# Found embeddings for 30.05% of vocab (without text cleaning)\n",
    "# Found embeddings for  87.66% of all text (without text cleaning)\n",
    "\n",
    "# Found embeddings for 46.85% of vocab (after text cleaning)\n",
    "# Found embeddings for  96.69% of all text (after text cleaning)\n",
    "\n",
    "wiki_embed_path = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "embeddings_dict_master = {}\n",
    "f = open(wiki_embed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "27c52fa0ef9e625deae490b2f81f8408b8f7e520"
   },
   "outputs": [],
   "source": [
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_dict_master[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1cd9798658eb2a7d31a78ba3d5e56541acc4d372"
   },
   "outputs": [],
   "source": [
    "import operator \n",
    "\n",
    "def check_coverage(vocab, embeddings_dict_master):\n",
    "    a = {}\n",
    "    oov = {}\n",
    "    k = 0\n",
    "    i = 0\n",
    "    for word in tqdm(vocab):\n",
    "        try:\n",
    "            #a[word] = word2vecDict[word]\n",
    "            a[word] = embeddings_dict_master[word]\n",
    "            k += vocab[word]\n",
    "        except:\n",
    "\n",
    "            oov[word] = vocab[word]\n",
    "            i += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n",
    "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "954d2215cb4670a527263f9935b1dfbd145c6fdc"
   },
   "outputs": [],
   "source": [
    "oov = check_coverage(vocab, embeddings_dict_master)\n",
    "oov[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4bd55473a7dcf4f27eb5d499272ad41c11ec20c0"
   },
   "outputs": [],
   "source": [
    "# Insincere questions are 93.8%\n",
    "\n",
    "sns.countplot(x='target', data=train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8dbd5bc802be6dc826c4ca29e55ae117634603ce"
   },
   "outputs": [],
   "source": [
    "train_X = train_df[\"question_text\"].values\n",
    "#val_X = val_df[\"question_text\"].values\n",
    "test_X = test_df[\"question_text\"].values\n",
    "\n",
    "train_y = train_df['target'].values\n",
    "#val_y = val_df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a90a6901ee03fccda9faa6005c9636103800cc7d"
   },
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4e20071144800c62e90542f449d84081897c123a"
   },
   "outputs": [],
   "source": [
    "# Feed all questions from train, val & test\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_tokens)\n",
    "#tokenizer.fit_on_texts(list(train_X) + list(val_X) + list(test_X))\n",
    "tokenizer.fit_on_texts(list(train_X) + list(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "001d7c7227e83dcd8b16e38884af9448a08da167"
   },
   "outputs": [],
   "source": [
    "# preparing the FastText word-embeddings matrix\n",
    "embedding_matrix = np.zeros((max_tokens, EMBED_DIM))\n",
    "word_index = tokenizer.word_index\n",
    "for word, i in word_index.items():\n",
    "    if i < max_tokens:\n",
    "        embedding_vector = embeddings_dict_master.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector # words not found will be all zeroes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5263bce226c35eaaaa401a184b68b489d33cb32e"
   },
   "outputs": [],
   "source": [
    "train_X = tokenizer.texts_to_sequences(train_X)\n",
    "#val_X = tokenizer.texts_to_sequences(val_X)\n",
    "test_X = tokenizer.texts_to_sequences(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "09e623b2baa6971e4444e78d0a69dcdb494d0072"
   },
   "outputs": [],
   "source": [
    "train_X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ee03604b34e60d33e574b8dfd92f8f989ad28d1b"
   },
   "outputs": [],
   "source": [
    "mylen = np.vectorize(len)\n",
    "len_train = mylen(train_X)\n",
    "#len_val = mylen(val_X)\n",
    "len_test = mylen(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cbff0fad234226123eb323e8d07ba0d03e6a6994"
   },
   "outputs": [],
   "source": [
    "# NOTE that Seaborn distplot does not support log scale :(\n",
    "\n",
    "sns.distplot( mylen(train_X) , kde=False, color=\"skyblue\", label=\"train_X\")\n",
    "#sns.distplot( mylen(val_X) , kde=False, color=\"red\", label=\"val_X\")\n",
    "sns.distplot( mylen(test_X) , kde=False, color=\"green\", label=\"test_X\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ac9de90923a68321660bbb12c0bf996629938f3b"
   },
   "outputs": [],
   "source": [
    "#unique_elements, counts_elements = np.unique(len_train, return_counts=True)\n",
    "#print(np.asarray((unique_elements, counts_elements)))\n",
    "\n",
    "# Keep MAX_SEQ_LEN at 30 as 30+ test questions are just 0.03%\n",
    "#(sum(len_train > 30)*100/train_df.shape[0], sum(len_val > 30)*100/train_df.shape[0], sum(len_test > 30)*100/test_df.shape[0])\n",
    "(sum(len_train > 30)*100/train_df.shape[0], sum(len_test > 30)*100/test_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f2a90313864484972f011e5e0619c5753e4923e6"
   },
   "outputs": [],
   "source": [
    "train_X = pad_sequences(train_X, maxlen=MAX_SEQ_LEN)\n",
    "#val_X = pad_sequences(val_X, maxlen=MAX_SEQ_LEN)\n",
    "test_X = pad_sequences(test_X, maxlen=MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "663bf8bfbb989d89ac6f5a3b4204ff388822caa7"
   },
   "outputs": [],
   "source": [
    "# CuDNNGRU is ~6-7 times faster than GRU/LSTM on GPU\n",
    "\n",
    "def model_BidiGruLstm_Attention(embedding_matrix):\n",
    "    inp = Input(shape=(MAX_SEQ_LEN,))\n",
    "    x = Embedding(max_tokens, EMBED_DIM, weights=[embedding_matrix])(inp)\n",
    "    x = Bidirectional(CuDNNGRU(LSTM_UNITS, return_sequences=True))(x)\n",
    "    \n",
    "    x = Attention(MAX_SEQ_LEN)(x)\n",
    "    \n",
    "    x = Dense(16, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    \n",
    "    # Decay rate momentum rates (beta_1=0.9, beta_2=0.999), epsilon=None, decay=0.0, amsgrad=False\n",
    "    adam = optimizers.Adam(lr=LEARNING_RATE)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    \n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e4c979df0fb2805ec11534c5e34f1a9ffbc85850"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "tmp_path = Path(\"../tmp\")\n",
    "\n",
    "if not tmp_path.is_dir():\n",
    "    print(\"tmp folder is not available, hence created\")\n",
    "    os.mkdir(tmp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7bfc58b88c35a4a64bd56fe664c9fbbd6c4dfd66"
   },
   "outputs": [],
   "source": [
    "filepath = \"../tmp/weights.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0b86fffd1d5445a243f8c8f7a0c0177c6ae83d2f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = model_BidiGruLstm_Attention(embedding_matrix)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "44cfa0fd113b0f37b5e579661056a6d42039cb22"
   },
   "outputs": [],
   "source": [
    "model_fitting_history = model.fit(train_X, train_y, validation_split=VALID_TRAIN_RATIO, epochs=N_EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks_list, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6d4143ea1d64758933228d8b4e85370babb7d93c"
   },
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.subplot(211)\n",
    "plt.plot(np.arange(1, N_EPOCHS+1), model_fitting_history.history['acc'])\n",
    "plt.plot(np.arange(1, N_EPOCHS+1), model_fitting_history.history['val_acc'])\n",
    "plt.title('model val_accuracy')\n",
    "plt.ylabel('validation accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validate'], loc='upper left')\n",
    "\n",
    "plt.figure(2)\n",
    "plt.subplot(212)\n",
    "plt.plot(np.arange(1, N_EPOCHS+1), model_fitting_history.history['loss'])\n",
    "plt.plot(np.arange(1, N_EPOCHS+1), model_fitting_history.history['val_loss'])\n",
    "plt.title('model val_loss')\n",
    "plt.ylabel('validation loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validate'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a48a35a331b66657f776d4114d8a6f32fd3ddcd3"
   },
   "outputs": [],
   "source": [
    "# Recreate the original network model without above learned weights, so that best learned weights can be loaded into it for Inference, i.e., Prediction\n",
    "\n",
    "model_final = model_BidiGruLstm_Attention(embedding_matrix)\n",
    "model_final.load_weights(\"../tmp/weights.best.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "92020ba22d7817ed8f835c1c1d0e1167d861a39d"
   },
   "outputs": [],
   "source": [
    "pred_test_y = model_final.predict([test_X], batch_size=BATCH_SIZE, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2d192dc0fb1ea54a6201123df7050f5fd044aa04"
   },
   "outputs": [],
   "source": [
    "pred_test_y = np.round(pred_test_y).astype(int)\n",
    "out_df = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\n",
    "out_df['prediction'] = pred_test_y\n",
    "out_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

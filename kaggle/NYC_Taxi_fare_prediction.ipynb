{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4363a106cf54eb6a7972c9b0e9dcc03f8d99c201"
   },
   "source": [
    "## NYC Taxi Fare Prediction: EDA, DBSCAN GeoSpatial Clustering & Regression Modeling\n",
    "#### Sunil Kumar\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "564fa4a3044821c473559332928617b98efd8ecd"
   },
   "source": [
    "## Decision Summary\n",
    "\n",
    "My primary interest in solving this Playground Competition was to closely experience the challenes of working with Latitude & Longitude data, introduce pickups & dropoffs desnity feature and asses its impact on NYC Taxi Fare Prediction. The geospatial clustered density of pickup & dropoff has been estimated using DBSCAN algorithm with predefined maximum cluster radius 0.5 km (EPS_IN_KM) and minimum pickup/dropoff count 500 (MIN_SAMPLES_CLUSTER).\n",
    "\n",
    "The purpose of choosing DBSCAN [ref. 1] was to identify low-density pickups/dropoffs locations which DBSCAN calls Outliers. Note that the other popular clustering algorithm K-Means determines k (predefined number) centroids which would not serve the purpose due to irregular geospatial distribution. Prior to hitting upon Clustering & specifically DBSCAN, I explored the options of geospatial 2-D binning (tiles, hexbin, etc.) but they are more useful for visualization and not so suitable for my problem statement & sparse dataset.\n",
    "\n",
    "The 'train' data pruning has been performed to ensure that original + engineered features' {domain} are not compromised in 'test' - refer to comments at the beginning of code blocks in Data Cleaning & Feature Engineering sections. Minimal data clearning [ref. 5] has been done to get a descent working dataset.\n",
    "\n",
    "## Useful Insights on DBSCAN\n",
    "\n",
    "* Refer to [ref. 1] for consice explanation.\n",
    "* Refer to [ref. 2] for pros & cons of DBSCAN. NOTE that most of its cons are not applicable to geospatial use cases.\n",
    "* Though DBSCAN has a worst-case runtime comlexity of O(n²), its database-oriented range-query formulation of DBSCAN allows for index acceleration for better performance upto O(n log n).\n",
    "* Dealing with large geospatial dataset is quite a challenge, hence realistic datasets would necessarily need Big Data implementation of this algorithm [ref. 3].\n",
    "* Sklearnc.luster.DBSCAN performance compares very well with repsect to other alternatives [ref. 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4f19c5cd595bdf33a8d5d3c7967295ece322bc51"
   },
   "source": [
    "## Global Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5cb47f36899f6bf71f5eceab7b85c0051b4cfc70"
   },
   "outputs": [],
   "source": [
    "##############################################################\n",
    "# Constants\n",
    "##############################################################\n",
    "\n",
    "KMS_PER_RADIAN = 6371.0088\n",
    "\n",
    "JFK_GEO_LOCATION = (40.6413, -73.7781)\n",
    "LGR_GEO_LOCATION = (40.7769, -73.8740)\n",
    "EWR_GEO_LOCATION = (40.6895, -74.1745)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "de7a3956bc09a9544e18d747cf51f7b32a3826d2"
   },
   "outputs": [],
   "source": [
    "##############################################################\n",
    "# Input Parameters used with 1M training data points\n",
    "##############################################################\n",
    "\n",
    "# Training data rows to read\n",
    "MAX_TRAINING_SIZE = 1_000_00\n",
    "\n",
    "# Input parameters for DBSCAN GeoSpatial Desnity based clustering\n",
    "EPS_IN_KM = 0.1           ## NOTE that lat/long are available till 5th decimal value & 0.1km = 1.xe-5, hence avoid using smaller DBSCAN's eps, i.e., radius threshold for clustering\n",
    "MIN_SAMPLES_CLUSTER = 500\n",
    "\n",
    "# Pickup/dropoff within small radius of airports geo location\n",
    "RADIUS_VICINITY_AIRPORTS = 1.0\n",
    "\n",
    "# Thershold for trip fare rate to remove those spurious trips involving exorbitant fare rate\n",
    "THERSHOLD_TRIP_FARE_RATE = 50.0\n",
    "\n",
    "# Thereshold for compressing trip distance range from 0.0-110.x to 0.0-25.0\n",
    "THRESHOLD_TRIP_DISTANCE = 25.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "% matplotlib inline\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "\n",
    "import timeit\n",
    "from sklearn import metrics\n",
    "from haversine import haversine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3f1406683d16b6554ab4915985c73da02c7760c4"
   },
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "# read data in pandas dataframe\n",
    "df_train =  pd.read_csv('../input/train.csv', nrows = MAX_TRAINING_SIZE, parse_dates=[\"pickup_datetime\"])\n",
    "df_holdout =  pd.read_csv('../input/test.csv', parse_dates=[\"pickup_datetime\"])\n",
    "test_key = df_holdout['key']\n",
    "df_train.drop(columns = ['key'], inplace=True)\n",
    "df_holdout.drop(columns = ['key'], inplace=True)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "elapsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4ddea018c70d2ba1c0905151b2ba4745ae15642d"
   },
   "source": [
    "## Naive Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0e5832930db78db07fe111bba8b6676c7d80d42f"
   },
   "outputs": [],
   "source": [
    "print('Old size: %d' % len(df_train))\n",
    "\n",
    "### Ignore -ve fare\n",
    "df_train = df_train[df_train.fare_amount >=0]\n",
    "\n",
    "### NOTE that there is no missing or NA in 'test'\n",
    "### Remove rows with NA in any field\n",
    "df_train = df_train.dropna(how='any', axis='rows')\n",
    "\n",
    "### 'test': No spurious passenger_count (min is 1 & max is 6)\n",
    "### 'train': passengers_count max is 208... just 11 out of 1M trips with count > 7, hence removing those trips\n",
    "df_train = df_train.drop(index= df_train[df_train.passenger_count >= 7].index, axis='rows')\n",
    "df_train = df_train.drop(index= df_train[df_train.passenger_count == 0].index, axis='rows')\n",
    "\n",
    "print('New size: %d' % len(df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "590931b8e9b6128347c3e5d1ae592457252a6ba1"
   },
   "outputs": [],
   "source": [
    "### NOTE that 'test' lat-long are well within NYC boundary, whereas there are few spurious 'train' datapoints outside of NYC boundary hence removing those trips\n",
    "\n",
    "#min(df_train.pickup_longitude.min(), df_train.dropoff_longitude.min()), max(df_train.pickup_longitude.max(), df_train.dropoff_longitude.max())\n",
    "#min(df_train.pickup_latitude.min(), df_train.dropoff_latitude.min()), max(df_train.pickup_latitude.max(), df_train.dropoff_latitude.max())\n",
    "\n",
    "def select_within_boundingbox(df, BB):\n",
    "    return (df.pickup_longitude >= BB[0]) & (df.pickup_longitude <= BB[1]) & \\\n",
    "           (df.pickup_latitude >= BB[2]) & (df.pickup_latitude <= BB[3]) & \\\n",
    "           (df.dropoff_longitude >= BB[0]) & (df.dropoff_longitude <= BB[1]) & \\\n",
    "           (df.dropoff_latitude >= BB[2]) & (df.dropoff_latitude <= BB[3])\n",
    "            \n",
    "#Times Square (40.7590° N, 73.9845° W)\n",
    "#BB = (-74.2, -73.8, 40.6, 41.0)\n",
    "\n",
    "# load image of NYC map\n",
    "BB = (-74.5, -72.8, 40.5, 41.8)\n",
    "\n",
    "print('Old size: %d' % len(df_train))\n",
    "df_train = df_train[select_within_boundingbox(df_train, BB)]\n",
    "print('New size: %d' % len(df_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c1e33495c9fa3e76d5893b590d06aadd941feb44"
   },
   "source": [
    "## EDA, Feature Engineering & Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": false,
    "_uuid": "c867782140f055cdd0d93be59f1e0db4605580f2"
   },
   "outputs": [],
   "source": [
    "# IDEA.n: Ideally, shortest route distance should be used\n",
    "def addPickDropDistanceFeature(df):\n",
    "\n",
    "    df['trip_distance'] = df.apply(\n",
    "        (lambda row: haversine(\n",
    "            (row['pickup_latitude'], row['pickup_longitude']),\n",
    "            (row['dropoff_latitude'], row['dropoff_longitude']))\n",
    "        ),\n",
    "        axis='columns'\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def addAirportDistanceFeatures(df):\n",
    "\n",
    "    df['pickup_distance_to_jfk'] = df.apply(\n",
    "        (lambda row: haversine(\n",
    "            (row['pickup_latitude'], row['pickup_longitude']),\n",
    "            (JFK_GEO_LOCATION[0], JFK_GEO_LOCATION[1]))\n",
    "        ),\n",
    "        axis='columns'\n",
    "    )\n",
    "\n",
    "    df['drop_distance_to_jfk'] = df.apply(\n",
    "        (lambda row: haversine(\n",
    "            (row['dropoff_latitude'], row['dropoff_longitude']),\n",
    "            (JFK_GEO_LOCATION[0], JFK_GEO_LOCATION[1]))\n",
    "        ),\n",
    "        axis='columns'\n",
    "    )\n",
    "\n",
    "    df['pickup_distance_to_lgr'] = df.apply(\n",
    "        (lambda row: haversine(\n",
    "            (row['pickup_latitude'], row['pickup_longitude']),\n",
    "            (LGR_GEO_LOCATION[0], LGR_GEO_LOCATION[1]))\n",
    "        ),\n",
    "        axis='columns'\n",
    "    )\n",
    "\n",
    "    df['drop_distance_to_lgr'] = df.apply(\n",
    "        (lambda row: haversine(\n",
    "            (row['dropoff_latitude'], row['dropoff_longitude']),\n",
    "            (LGR_GEO_LOCATION[0], LGR_GEO_LOCATION[1]))\n",
    "        ),\n",
    "        axis='columns'\n",
    "    )\n",
    "\n",
    "    df['pickup_distance_to_ewr'] = df.apply(\n",
    "        (lambda row: haversine(\n",
    "            (row['pickup_latitude'], row['pickup_longitude']),\n",
    "            (EWR_GEO_LOCATION[0], EWR_GEO_LOCATION[1]))\n",
    "        ),\n",
    "        axis='columns'\n",
    "    )\n",
    "\n",
    "    df['drop_distance_to_ewr'] = df.apply(\n",
    "        (lambda row: haversine(\n",
    "            (row['dropoff_latitude'], row['dropoff_longitude']),\n",
    "            (EWR_GEO_LOCATION[0], EWR_GEO_LOCATION[1]))\n",
    "        ),\n",
    "        axis='columns'\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def getAirportTrips(df, airportVicinity):\n",
    "    ids = (df.pickup_distance_to_jfk < airportVicinity) | (df.drop_distance_to_jfk < airportVicinity) | (df.pickup_distance_to_lgr < airportVicinity) | (df.drop_distance_to_lgr < airportVicinity) | (df.pickup_distance_to_ewr < airportVicinity) | (df.drop_distance_to_ewr < airportVicinity)\n",
    "    \n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "58b559ac9fd6971674fcbd65b72bc1580be6e09a"
   },
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "# Add pickup-dropoff distance feature\n",
    "df_train = addPickDropDistanceFeature(df_train)\n",
    "df_holdout = addPickDropDistanceFeature(df_holdout)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0e1cfe6897ebe027a52909cf92b18fbbbd082cb2"
   },
   "outputs": [],
   "source": [
    "# With 1M datapoints, trip_distance range can be compressed from 0.0-110.83 to 0.0-25.0\n",
    "# which would drop just 690 & 11 training & testing datapoints, i.e., \n",
    "# worst case impact on prediction accuracy by 0.1% (11/test_size*100)\n",
    "\n",
    "bucketsCount = 100\n",
    "feat = 'trip_distance'\n",
    "\n",
    "df_train[feat].hist(bins=bucketsCount, figsize = (15,8))\n",
    "df_holdout[feat].hist(bins=bucketsCount, figsize = (15,8))\n",
    "plt.yscale('log')\n",
    "plt.xlabel(feat)\n",
    "plt.ylabel(\"Frequency Log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "abaeeeb8d8ccfab4d413f6f148c8cad25d9eaaab"
   },
   "outputs": [],
   "source": [
    "#(len(df_train[df_train[feat] > THRESHOLD_TRIP_DISTANCE]), len(df_holdout[df_holdout[feat] > THRESHOLD_TRIP_DISTANCE]))\n",
    "\n",
    "print('Old size: %d' % len(df_train))\n",
    "df_train = df_train[df_train.trip_distance < THRESHOLD_TRIP_DISTANCE]\n",
    "print('New size: %d' % len(df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "19595730b1ea6b6448ef677999859770d33bb4f2"
   },
   "outputs": [],
   "source": [
    "def add_datetime_features(df):\n",
    "    #Convert to datetime format\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'],format=\"%Y-%m-%d %H:%M:%S UTC\")\n",
    "    \n",
    "    df['hour'] = df.pickup_datetime.dt.hour\n",
    "    df['day'] = df.pickup_datetime.dt.day\n",
    "    df['month'] = df.pickup_datetime.dt.month\n",
    "    df['weekday'] = df.pickup_datetime.dt.weekday\n",
    "    df['year'] = df.pickup_datetime.dt.year\n",
    "    \n",
    "    return df\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "df_train = add_datetime_features(df_train)\n",
    "df_holdout = add_datetime_features(df_holdout)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6544019409d050115664a571fd7bddfa3ca37c2f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hour_bins = [-1, 5, 7, 10, 16, 21, 23]\n",
    "bin_names = ['late_night', 'morning', 'morning_peak', 'afternoon', 'evening', 'night']\n",
    "df_train['hour_type'] = pd.cut(df_train.hour, bins=hour_bins, labels=bin_names).cat.codes\n",
    "df_holdout['hour_type'] = pd.cut(df_train.hour, bins=hour_bins, labels=bin_names).cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1080b593ebd2e484061acd195fe5142f266c2032"
   },
   "outputs": [],
   "source": [
    "# Merging both 'train' & 'holdout' for common feature engineering afterwhich 'holdout' data will be extracted\n",
    "train_len = len(df_train)\n",
    "df_nyc_taxi = pd.concat([df_train, df_holdout], axis=0, ignore_index=False, sort=False)\n",
    "#df_nyc_taxi.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d1fdf9bf2e4e0a5bb9cc9ac3fdd08869dab90fdc"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "EPS_IN_RADIAN = EPS_IN_KM / KMS_PER_RADIAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "089e908a9040b917e588304fc920a51832e23190"
   },
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "dbscan_pick = DBSCAN(eps=EPS_IN_RADIAN, min_samples=MIN_SAMPLES_CLUSTER, algorithm='ball_tree', metric='haversine').fit(np.radians(df_nyc_taxi.loc[:,'pickup_longitude':'pickup_latitude']))\n",
    "labels_pick = dbscan_pick.labels_\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fb9c246cb935e86daffc49ce413d44a9637e554d"
   },
   "outputs": [],
   "source": [
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_pick = len(set(labels_pick)) - (1 if -1 in labels_pick else 0)\n",
    "n_clusters_pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0f6b5eaee05fbf0cec3dc897ff683fd617d18022"
   },
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "dbscan_drop = DBSCAN(eps=EPS_IN_RADIAN, min_samples=MIN_SAMPLES_CLUSTER, algorithm='ball_tree', metric='haversine').fit(np.radians(df_nyc_taxi.loc[:,'dropoff_longitude':'dropoff_latitude']))\n",
    "labels_drop = dbscan_drop.labels_\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "312a9dabdaa5b8b86ed339ea163a31097661deb4"
   },
   "outputs": [],
   "source": [
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_drop = len(set(labels_drop)) - (1 if -1 in labels_drop else 0)\n",
    "n_clusters_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "be149b59280e1939fd36a715ddf84a00f942d2b2"
   },
   "outputs": [],
   "source": [
    "df_nyc_taxi['density_DBSCAN_pickup'] = labels_pick\n",
    "df_nyc_taxi['density_DBSCAN_dropoff'] = labels_drop\n",
    "#df_nyc_taxi['dense_DBSCAN_trips'] = ((labels_pick != -1) & (labels_drop != -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a7d34f7d503c7cc9b9cdf3e3923974b02f79f22e"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# NOTE that our focus of DBSCAN is not to differentiate levels of clusters, i.e., set of connected clusters => hence we are plotting all clusters together\n",
    "df_tmp = df_nyc_taxi.loc[df_nyc_taxi.dense_DBSCAN_trips == 1]\n",
    "plt.plot(df_tmp.pickup_longitude, df_tmp.pickup_latitude, 'o')\n",
    "plt.xlabel(\"Pickup Longitude\")\n",
    "plt.ylabel(\"Pickup Latitude\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "832a6f6c97196d4aba65b7c122b32104bb1dc807"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "plt.plot(df_tmp.dropoff_longitude, df_tmp.dropoff_latitude, 'o')\n",
    "plt.xlabel(\"Dropoff Longitude\")\n",
    "plt.ylabel(\"Dropoff Latitude\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4ea1026a0a89c18e1fad25e4d8dc1924c87fb3c3"
   },
   "outputs": [],
   "source": [
    "df_train = df_nyc_taxi.iloc[:train_len, :]\n",
    "df_holdout = df_nyc_taxi.iloc[train_len:, :].iloc[:, df_nyc_taxi.columns != 'fare_amount']\n",
    "\n",
    "(len(df_train), len(df_holdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d8f9a53158d5f65ad0763dc5c4bf80e0cc48e820"
   },
   "outputs": [],
   "source": [
    "# Ceiling near-zero fare values to 0.2 to check fare/dist behavior\n",
    "df_train.loc[df_train.trip_distance < 0.2, 'trip_distance'] = 0.2\n",
    "\n",
    "(df_train.fare_amount / df_train.trip_distance).hist(bins=bucketsCount, figsize = (15,8))\n",
    "plt.yscale('log')\n",
    "plt.xlabel('trip_rate')\n",
    "plt.ylabel(\"Log Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "30e6ffcda33cbed3e8d30fe34bbb500028d8cca6"
   },
   "outputs": [],
   "source": [
    "df_train['trip_rate'] = df_train.apply(\n",
    "    (lambda row: (row.fare_amount / row.trip_distance)),\n",
    "    axis='columns'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6dbd347c8b406a9e6768a0aec62c208647f7b306"
   },
   "outputs": [],
   "source": [
    "#len(df_train.loc[df_train.trip_rate > THERSHOLD_TRIP_FARE_RATE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5e2ca44cd5f725ec7f900dd5f9a553a42f99b92d"
   },
   "outputs": [],
   "source": [
    "#Trying to check if not removing 50+ trip_rate improve the score!\n",
    "ids = (df_train.trip_rate < THERSHOLD_TRIP_FARE_RATE)\n",
    "\n",
    "print('Old size: %d' % len(df_train))\n",
    "df_train = df_train[ids]\n",
    "print('New size: %d' % len(df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ab5aedf745d4a644d8f2f60e28480fb4dbda66c3"
   },
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "# Add airport trips distance features\n",
    "df_train = addAirportDistanceFeatures(df_train)\n",
    "df_holdout = addAirportDistanceFeatures(df_holdout)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "23d6e3b10d0f114d4b7dc5eb860977266d9319b1"
   },
   "outputs": [],
   "source": [
    "#df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a9396397bfcb121c4cb867166e0a6041e98c7213"
   },
   "outputs": [],
   "source": [
    "# Split training data into Airport & City trips\n",
    "\n",
    "airportTripsIds = getAirportTrips(df_holdout, RADIUS_VICINITY_AIRPORTS)\n",
    "df_holdout['airport_bound'] = airportTripsIds\n",
    "\n",
    "airportTripsIds = getAirportTrips(df_train, RADIUS_VICINITY_AIRPORTS)\n",
    "df_train['airport_bound'] = airportTripsIds\n",
    "df_airport_trips = df_train.loc[airportTripsIds]\n",
    "df_city_trips = df_train.loc[-airportTripsIds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6edb273ad0ec6ba77dd3702453c162fd81621559"
   },
   "outputs": [],
   "source": [
    "# Compare trip_rate for Airport & City trips\n",
    "pd.DataFrame(data={'Airport Trips' : df_airport_trips.trip_rate, 'City Trips' : df_city_trips.trip_rate}).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5a233fc0ed0b246e7c7e750fca218c088a6ddbdd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compare trip_rate for good -vs- poorly dense trips\n",
    "#pd.DataFrame(data={'Good Density Trips' : df_train.loc[df_train.dense_DBSCAN_trips == 1].trip_rate, 'LOW Density Pickups' : df_train.loc[df_train.dense_DBSCAN_trips == 0].trip_rate}).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7d61ae894b6384d1412360b60600966f62400be4"
   },
   "outputs": [],
   "source": [
    "df_train = df_train.drop(columns = ['pickup_datetime', 'pickup_distance_to_jfk', 'drop_distance_to_jfk', 'pickup_distance_to_lgr', 'drop_distance_to_lgr', 'pickup_distance_to_ewr', 'drop_distance_to_ewr', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'trip_rate', 'hour'])\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "373e251bcf61d31059bff8a1e1dc1dc09ba088f7"
   },
   "outputs": [],
   "source": [
    "y = df_train['fare_amount']\n",
    "train = df_train.drop(columns=['fare_amount'])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(train, y, random_state=0, test_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3b96c9fe00cf23ae1aaab8bd3bfa3cb4669ef65d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Cross-validation\n",
    "params = {\n",
    "    # Parameters that we are going to tune.\n",
    "    'max_depth': 8, #Result of tuning with CV\n",
    "    'eta':.03, #Result of tuning with CV\n",
    "    'subsample': 1, #Result of tuning with CV\n",
    "    'colsample_bytree': 0.8, #Result of tuning with CV\n",
    "    # Other parameters\n",
    "    'objective':'reg:linear',\n",
    "    'eval_metric':'rmse',\n",
    "    'silent': 1\n",
    "}\n",
    "\n",
    "#Block of code used for hypertuning parameters. Adapt to each round of parameter tuning.\n",
    "#Turn off CV in submission\n",
    "CV=False\n",
    "if CV:\n",
    "    dtrain = xgb.DMatrix(train,label=y)\n",
    "    gridsearch_params = [\n",
    "        (eta)\n",
    "        for eta in np.arange(.04, 0.12, .02)\n",
    "    ]\n",
    "\n",
    "    # Define initial best params and RMSE\n",
    "    min_rmse = float(\"Inf\")\n",
    "    best_params = None\n",
    "    for (eta) in gridsearch_params:\n",
    "        print(\"CV with eta={} \".format(\n",
    "                                 eta))\n",
    "\n",
    "        # Update our parameters\n",
    "        params['eta'] = eta\n",
    "\n",
    "        # Run CV\n",
    "        cv_results = xgb.cv(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=1000,\n",
    "            nfold=3,\n",
    "            metrics={'rmse'},\n",
    "            early_stopping_rounds=10\n",
    "        )\n",
    "\n",
    "        # Update best RMSE\n",
    "        mean_rmse = cv_results['test-rmse-mean'].min()\n",
    "        boost_rounds = cv_results['test-rmse-mean'].argmin()\n",
    "        print(\"\\tRMSE {} for {} rounds\".format(mean_rmse, boost_rounds))\n",
    "        if mean_rmse < min_rmse:\n",
    "            min_rmse = mean_rmse\n",
    "            best_params = (eta)\n",
    "\n",
    "    print(\"Best params: {}, RMSE: {}\".format(best_params, min_rmse))\n",
    "else:\n",
    "    #Print final params to use for the model\n",
    "    params['silent'] = 0 #Turn on output\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6516ece5c786c65d898282f479597dbe1c6caca3"
   },
   "outputs": [],
   "source": [
    "def XGBmodel(x_train,x_test,y_train,y_test,params):\n",
    "    matrix_train = xgb.DMatrix(x_train,label=y_train)\n",
    "    matrix_test = xgb.DMatrix(x_test,label=y_test)\n",
    "    model=xgb.train(params=params,\n",
    "                    dtrain=matrix_train,num_boost_round=5000, \n",
    "                    early_stopping_rounds=10,evals=[(matrix_test,'test')])\n",
    "    return model\n",
    "\n",
    "model = XGBmodel(x_train,x_test,y_train,y_test,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "35324eb9603ded412659ee3fb8bcec827d1a586d"
   },
   "outputs": [],
   "source": [
    "#Read and preprocess test set\n",
    "x_pred = df_holdout.drop(columns = ['pickup_datetime', 'pickup_distance_to_jfk', 'drop_distance_to_jfk', 'pickup_distance_to_lgr', 'drop_distance_to_lgr', 'pickup_distance_to_ewr', 'drop_distance_to_ewr', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'hour'])\n",
    "\n",
    "#Predict from test set\n",
    "prediction = model.predict(xgb.DMatrix(x_pred), ntree_limit = model.best_ntree_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "93cc96f21ee4d92646c3633af5828a900628e2a2"
   },
   "outputs": [],
   "source": [
    "len(test_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1f840c4fcfce8e792f782d33988923a28e81bb80"
   },
   "outputs": [],
   "source": [
    "#Create submission file\n",
    "submission = pd.DataFrame({\n",
    "        \"key\": test_key,\n",
    "        \"fare_amount\": prediction.round(2)\n",
    "})\n",
    "\n",
    "submission.to_csv('taxi_fare_submission.csv',index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e126d84e02aee7f5eb858e8b8ad2e40f985e77bf"
   },
   "source": [
    "## References\n",
    "\n",
    "1. [Density based Clustering by Manojit Nandi, Domino Data Lab](https://blog.dominodatalab.com/topology-and-density-based-clustering/)\n",
    "2. [DBSCAN Wiki](https://en.wikipedia.org/wiki/DBSCAN)\n",
    "3. [Efficient Large Scale Clustering based on Data Partitioning](https://arxiv.org/pdf/1704.03421.pdf)\n",
    "4. [Benchmarking Performance and Scaling of Python Clustering Algorithms](https://hdbscan.readthedocs.io/en/latest/performance_and_scalability.html)\n",
    "5. [Kaggle kernel for Data Exploration](https://www.kaggle.com/breemen/nyc-taxi-fare-data-exploration/notebook)\n",
    "6. [Kaggle kernel for XGBoost](https://www.kaggle.com/gunbl4d3/xgboost-ing-taxi-fares)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
